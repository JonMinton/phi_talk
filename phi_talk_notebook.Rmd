---
title: "The Infer Package and Bayes Factors"
author: "Jon Minton"
output:
  html_document:
    df_print: paged
---

# Introduction 

This document will introduce a couple of concepts, and one R package, for performing statistical inference tests in R.

1. The There-is-only-one-test framework, and the R package infer
2. Bayes Factors, as distinct from Neyman-Pearson Hypothesis Testing frameoworks

# Proposed structure

* TIOOT/Infer
    * There-is-only-one-test : background
        * Allen Downey - a computer scientist's take on statistical inference
            * 2011 - [There is only one test](http://allendowney.blogspot.com/2011/05/there-is-only-one-test.html)
            * 2016 - [There is still only one test](http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html)
            * ![Key Figure](figures/dcq7d5hs_237c9bcfngs_b.png)
        * Null & Alternative Hypothesis
            * Null $H_0$: The (boring) world in which the proposed relationship between predictor and response variables is *false*
            * Alternative $H_A$: The (interesting) world in which the proposed relationship between predictor and response is *true*
        * P-values 
            * > P-values are a measure of conflict between data and a hypothesis, and are certainly not direct expressions of a probability of hypotheses.
                * [Communicating Uncertainty about facts, numbers, and science](https://royalsocietypublishing.org/doi/pdf/10.1098/rsos.181870)
            * P-values are usually *indirect* strategies for investigating support for $H_A$, through quantifying *magnitude* and *direction* of conflict between data and $H_0$
                * *Direction of conflict*: One-tailed cf two-tailed tests
                * *Magnitude of conflict*: Probability of observing values of effect $\delta^{*}$ as or more extreme under $H_0$.
            * Neyman-Pearson Approach
                * Also prespecify $\alpha$: The magnitude of conflict between data and Null hypothesis, in expected direction, required to 'reject the Null'
                * Typical $\alpha$ values: 0.05, 0.01
                    * CERN: 'Five Sigma' (almost zero)
                * [The Sizeless Stare](https://www.amazon.co.uk/Cult-Statistical-Significance-Economics-Cognition/dp/0472050079)
            * Permutation approaches
                * A generic, computationally intensive, approach to producing a distribution of expected effect sizes under the Null
                * Key intuition:
                    * $H_A$ says $X$ predicts/causes $Y$
                    * Put another way:  Values of $X$ are *informative* as to values of $Y$
                    * Say $X$ is categorical, and can either be $A$ or $B$
                    * The data $D$ is a series of values of $Y$, with *labels* attached: the corresponding values of $X$
                    * Another way of expressing $H_A$ and $H_0$:
                        * $H_A$: These labels *matter* (Are informative of $Y$)
                        * $H_0$: These labels *don't matter* (Are not informative of $Y$)
                    * A corollary of $H_0$: If the labels don't matter, there's no harm (information lost) by reallocating them to $Y$ values at random
                    * Random reallocation: Permutation
                    * Repeat many times to estimate the Null distribution 
    * [Infer package](https://github.com/tidymodels/infer)
        * ![Key Figure](https://raw.githubusercontent.com/tidymodels/infer/master/figs/ht-diagram.png)
        * Verbs
            * `specify`
            * `hypothesize`
            * `generate`
            * `calculate`
    * DataCamp courses [(Learner beware)](https://www.buzzfeednews.com/article/daveyalba/datacamp-sexual-harassment-metoo-tech-startup)
        * [Foundations of Inference](https://www.datacamp.com/courses/foundations-of-inference)
        * [Inference for Linear Regression](https://www.datacamp.com/courses/inference-for-linear-regression)
        * [Inference for Numeric Data](https://www.datacamp.com/courses/inference-for-numerical-data)
        * [Inference for Categorical Data](https://www.datacamp.com/courses/inference-for-categorical-data)
    * [Avoiding Datacamp](https://bookdown.org/cteplovs/ismaykim/ismaykim.pdf)

# Examples of infer

```{r}
pacman::p_load(tidyverse, infer)

```

# Initial example: mtcars

* [mtcars dataset](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html)

## Variables of interest

* **vs**: Engine (0 - V-shaped; 1 = straight)
* **am**: Transmission (0 = automatic, 1 = manual)
* **mpg**: Miles per gallon

## Prep the data 
```{r}
mtcars <- as.data.frame(mtcars) %>%
  mutate(cyl = factor(cyl),
          vs = factor(vs),
          am = factor(am),
          gear = factor(gear),
          carb = factor(carb))
```

## First research question

* Does engine type influence(!) Transmission Type
* Null: It does not 
* Alternative: It does

```{r}
# Observed difference in proportions 
obs_diff_rq1 <- 
  mtcars %>% 
    select(am, vs) %>% 
    group_by(vs) %>% 
    summarise(prop = mean(am == 1)) %>% 
    summarise(diff_in_props = diff(prop)) %>% 
    pull(diff_in_props)

obs_diff_rq1

```

```{r}
# Null distribution: 
null_rq1 <- 
  mtcars %>%
    specify(am ~ vs, success = "1") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1000, type = "permute") %>%
    calculate(stat = "diff in props", order = c("1", "0"))
```

```{r}
# Visualise the Null

null_rq1 %>% 
  ggplot(aes(x = stat)) + 
  geom_histogram() + 
  geom_vline(xintercept = 0) + 
  geom_vline(xintercept = obs_diff_rq1, colour = "red", size = 2)
```

```{r}
# P-value (two sided)

null_rq1 %>% 
  summarise(p_val = mean(stat > obs_diff_rq1))
```

To answer the RQ in Neyman-Pearson terms, with alpha of 0.05, two-tailed:
> Is the above value either greater than 0.975, or less than 0.025? 

## Second research question

> Do manual transmission vehicles (from the 1970s) have higher MPG?

* Alternative: They do
* Null: They don't

```{r}
# Observed difference in means
obs_diff_rq2 <- 
  mtcars %>% 
    group_by(am) %>% 
    summarise(mean_mpg = mean(mpg)) %>% 
    summarise(diff_in_means = diff(mean_mpg)) %>% 
    pull(diff_in_means)

obs_diff_rq2
```

```{r}
null_rq2 <- 
  mtcars %>%
    specify(response = mpg, explanatory = am) %>%
    hypothesize(null = "independence") %>% 
    generate(reps = 1000, type = "permute") %>%
    calculate(stat = "diff in means", order = c("1", "0")) 
```

```{r}
# Visualise

null_rq2 %>% 
  ggplot(aes(x = stat)) + 
  geom_histogram(bins = 100) +
  geom_vline(xintercept = 0) + 
  geom_vline(xintercept = obs_diff_rq2, colour = "red", size = 2)
```

```{r}
# P-value (one-sided)

null_rq2 %>% 
  summarise(p_val = mean(stat > obs_diff_rq2))
```


To answer the RQ in Neyman-Pearson terms, with alpha of 0.05, two-tailed:

> Is the above value either greater than 0.975, or less than 0.025? 

```{r}
# RQ2 using the t-distribution 

obs_diff_rq2_t <- 
  mtcars %>% 
  t_test(mpg ~ am, order = c("1", "0"), alternative = "less")

mtcars %>%
  specify(response = mpg, explanatory = am) %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "t", order = c("1", "0")) %>% 
  visualize(method = "both") + 
  shade_p_value(obs_stat = obs_diff_rq2_t, direction = "greater") +
  geom_vline(xintercept = 1.96, colour = "darkgreen", size = 1.5, linetype = "dashed")


```

# Second Example (If we get time)

> RQ: Are males taller than females (on average)

* $H_0$: No!
* $H_A$: Yes!

## Data Source - Kaggle

* From [this page](https://www.kaggle.com/majidarif17/weight-and-heightcsv).
* Provenance unknown...
* Unit appears to be inches 

## Load and standardise data

```{r}
# pacman::p_load(readxl)
# height_data <- read_excel("data/height_data.xlsx")
height_data <- read_csv("data/kaggle_weight-height.csv")


tidy_height <- 
  height_data %>% 
    select(gender = Gender, height = Height) 

tidy_height %>% 
  ggplot(aes(x = height, group = gender, fill = gender)) + 
  geom_density(alpha = 0.4)
```

## Hypothesis test

```{r, eval = FALSE}
obs_mean_diff <- 
  tidy_height %>% 
    group_by(gender) %>% 
    summarise(mean_height = mean(height)) %>% 
    summarise(mean_diff = mean_height[gender == "Male"] - mean_height[gender == "Female"]) %>% 
    pull(mean_diff)

obs_mean_diff

tidy_height %>% 
  specify(height ~ gender) %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in means", order = c("Male", "Female")) %>% 
  visualize() # %>%
  # shade_p_value(
  #   obs_stat = obs_mean_diff,
  #   direction = "greater"
  # )

```

# Bayes Factors 

* Analogy: Courts and Burden of Proof
    * Criminal Courts: 'Beyond Reasonable Doubt' (Neyman-Pearson/Classical approach)
    * Civil Courts: 'Balance of Probabilities' (The Bayes Factor: Likelihood Ratio)
* Likelihood and Probability
    * > The likelihood of the model given the data is proportional to the probability of the data given the model
    * Likelihood is always relative rather than absolute
        * A model is never *likely*, just *more/less likely* than another model
* Bayes Factors
    * $B(H_A, H_0) = \frac{p(D | H_A)}{p(D | H_0)}$
    * B > 1 : More support for Alternative Hypothesis
    * B < 1 : More support for Null Hypothesis
    * How much more support?
        * < 3: 'anecdotal'
        * < 10: 'moderate'
        * < 30: 'strong'
        * etc

## Example using mtcars dataset 

> Given some value of MPG, what's the relative likelihood the car is manual (Alt) or automatic (Null) transmission?

```{r}
summaries <- 
  mtcars %>% 
    group_by(am) %>% 
    summarise(
      mean_mpg = mean(mpg), 
      sd_mpg   = sd(mpg)
    )

mu_manual    <- summaries %>% filter(am == 1) %>% pull(mean_mpg)
sd_manual    <- summaries %>% filter(am == 1) %>% pull(sd_mpg  )
mu_automatic <- summaries %>% filter(am == 0) %>% pull(mean_mpg)
sd_automatic <- summaries %>% filter(am == 0) %>% pull(sd_mpg  )


```

Bayes factor (Manual cf automatic) given MPG

```{r}
calc_bayes_factor <- function(value, mu_null, sd_null, mu_alt, sd_alt){
  dnorm(value, mean = mu_alt, sd = sd_alt) / dnorm(value, mean = mu_null, sd = sd_alt)
}

```


```{r}
bf_schedule <- 
  tibble(
    mpg_vals = seq(0, 50, by = 0.01)
  )  %>% 
    mutate(bf = map_dbl(mpg_vals, calc_bayes_factor, mu_null = mu_automatic, sd_null = sd_automatic, mu_alt = mu_manual, sd_alt = sd_manual )
    ) 

bf_schedule %>% 
  ggplot(aes(x = mpg_vals, y = bf)) + 
  geom_line() + 
  scale_y_log10() +
  geom_hline(yintercept = 1)


```


## Example using our heights data

```{r}
summaries <- 
  tidy_height %>% 
      group_by(gender) %>% 
      summarise(
        mean_height = mean(height),
        sd_height = sd(height)
      ) 

mu_male      <- summaries %>% filter(gender == "Male") %>% pull(mean_height)
sd_male      <- summaries %>% filter(gender == "Male") %>% pull(sd_height  )
mu_female    <- summaries %>% filter(gender == "Female") %>% pull(mean_height)
sd_female    <- summaries %>% filter(gender == "Female") %>% pull(sd_height  )


```


```{r}
bf_schedule <- 
  tibble(
    height_vals = seq(50, 90, by = 1)
  )  %>% 
    mutate(bf = map_dbl(height_vals, calc_bayes_factor, mu_null = mu_female, sd_null = sd_female, mu_alt = mu_male, sd_alt = sd_male)
    ) 

bf_schedule %>% 
  ggplot(aes(x = height_vals, y = bf)) + 
  geom_line() + 
  scale_y_log10() +
  geom_hline(yintercept = 1)


```

Finally, what's the relative likelihood that I'm male?

```{r}
my_height <- 1.7145 * 39.3701 # to get to height in inches

calc_bayes_factor(value = my_height, mu_null = mu_female, sd_null = sd_female, mu_alt = mu_male, sd_alt = sd_male)

```

So, from height alone, it's twice as likely I'm male than female. 

#Conclusion/Discussion

* There-is-only-one-test makes it easy to think through hypothesis testing
* The `infer` package makes it straightforward to apply this approach 
* Computationally intensive and analytic approaches do the same thing in different ways
* Classical (Neyman-Pearson style) hypothesis tests may not answer the question you want to ask
* Bayes Factors are an alterantive and often more pragmatic way of weighing the evidence